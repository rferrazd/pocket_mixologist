{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  /Users/robertagarcia/Desktop/learning/LangGraph/langgraph_personal/NodeInterrupt/.venv/bin/python -m pip uninstall [options] <package> ...\n",
      "  /Users/robertagarcia/Desktop/learning/LangGraph/langgraph_personal/NodeInterrupt/.venv/bin/python -m pip uninstall [options] -r <requirements file> ...\n",
      "\n",
      "no such option: -U\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall --quiet -U langgraph langchain-openai\n",
    "%pip install --quiet -U langgraph langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langgraph' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m version = \u001b[43mlanggraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'langgraph' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import langgraph\n",
    "version = langgraph.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-core\n",
      "Version: 0.3.41\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/robertagarcia/Desktop/learning/LangGraph/langgraph_personal/NodeInterrupt/.venv/lib/python3.12/site-packages\n",
      "Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, tenacity, typing-extensions\n",
      "Required-by: langchain-openai, langgraph, langgraph-checkpoint, langgraph-prebuilt\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langgraph version: 0.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9v/0z2cy6c546g6bvh2dzly39x80000gn/T/ipykernel_67075/848964410.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import get_distribution\n"
     ]
    }
   ],
   "source": [
    "import langgraph\n",
    "\n",
    "try:\n",
    "    version = langgraph.__version__\n",
    "except AttributeError:\n",
    "    from pkg_resources import get_distribution\n",
    "    version = get_distribution(\"langgraph\").version\n",
    "    from pkg_resources import get_distribution\n",
    "    version = get_distribution(\"langgraph\").version\n",
    "\n",
    "print(\"langgraph version:\", version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **React Agent** with NodeInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Image, display\n",
    "\n",
    "load_dotenv()  # Load environment variables from the .env file\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "# Set up the state\n",
    "from langgraph.graph import MessagesState, START\n",
    "\n",
    "# Set up the tool\n",
    "# We will have one real tool - a search tool\n",
    "# We'll also have one \"fake\" tool - a \"ask_human\" tool\n",
    "# Here we define any ACTUAL tools\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "@tool\n",
    "def my_tool(query: str):\n",
    "    \"\"\"Tool for later use\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    # Don't let the LLM know this though üòä\n",
    "    return f\"{query}\"\n",
    "\n",
    "\n",
    "tools = [my_tool]\n",
    "tool_node = ToolNode(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10e6f74d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10f518110>, root_client=<openai.OpenAI object at 0x10e1f08f0>, root_async_client=<openai.AsyncOpenAI object at 0x10c9a9070>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'my_tool', 'description': 'Tool for later use', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'AskHuman', 'description': 'Ask the human a question', 'parameters': {'properties': {'question': {'type': 'string'}}, 'required': ['question'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# We are going \"bind\" all tools to the model\n",
    "# We have the ACTUAL tools from above, but we also need a mock tool to ask a human\n",
    "# Since `bind_tools` takes in tools but also just tool definitions,\n",
    "# We can define a tool definition for `ask_human`\n",
    "\n",
    "class AskHuman(BaseModel):\n",
    "    \"\"\"Ask the human a question\"\"\"\n",
    "    question: str\n",
    "\n",
    "class LLMResponse(BaseModel):\n",
    "    \"\"\"Class to define the pydantic structure for the LLM response\"\"\"\n",
    "    next_question: str = Field(description = \"response from the LLM containing the next question to the user.\")\n",
    "    review: str = Field(description = \"Boolean value to decide whether to go to review node or not. \")\n",
    "\n",
    "model = ChatOpenAI(model = \"gpt-4o-mini\")\n",
    "model = model.bind_tools(tools + [AskHuman])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nodes and conditional edges\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    print(f\"\\n\\n<><><><><>\\nTHIS IS THE LAST MESSAGE: {last_message} + \\n<><><><><>\\n\\n\")\n",
    "    #print(f'Inside should_continue, this was the last message {type(last_message)}: {last_message}')\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return END\n",
    "    # If tool call is asking Human, we return that node\n",
    "    # You could also add logic here to let some system know that there's something that requires Human input\n",
    "    # For example, send a slack message, etc\n",
    "    elif last_message.tool_calls[0][\"name\"] == \"AskHuman\":\n",
    "        return \"ask_human\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"action\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    print(\"\\n\\nTHIS IS THE FULL RESPONSE FROM MODEL.INVOKE:\\n\", response, \"\\n\\n\" )\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    #print(f\"Inside model, response from model: {response}\")\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# We define a fake node to ask the human\n",
    "def ask_human(state):\n",
    "    #print(\"\\nInside ask_human node, the last mesage is:\\n\", state[\"messages\"][-1])\n",
    "    tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "   \n",
    "    tool_call_arguments = state[\"messages\"][-1].tool_calls[0][\"args\"][\"question\"]\n",
    "    #print(f\"Question for user (arguments): {tool_call_arguments}\")\n",
    "    question_for_user = interrupt(tool_call_arguments)\n",
    "    tool_message = [{\"tool_call_id\": tool_call_id, \"type\": \"tool\", \"content\": question_for_user}]\n",
    "    return {\"messages\": tool_message}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the graph\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "class MyState(MessagesState):\n",
    "      approved: bool  = False\n",
    "      \n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the three nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "workflow.add_node(\"ask_human\", ask_human)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# After we get back the human response, we go back to the agent\n",
    "workflow.add_edge(\"ask_human\", \"agent\")\n",
    "\n",
    "# Set up memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "# We add a breakpoint BEFORE the `ask_human` node so it never executes\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a professional cocktail designer.\n",
    "\n",
    "CRITICAL INSTRUCTION: ALWAYS use the AskHuman tool to ask questions. NEVER ask questions directly in your response text.\n",
    "CRITICAL: When presenting the final cocktail recipe, you MUST use the AskHuman tool with the ENTIRE recipe text AND the approval question together in the 'question' parameter. DO NOT generate a regular text message containing the recipe.\n",
    "\n",
    "Ask these questions ONE AT A TIME using the AskHuman tool:\n",
    "1. Do you prefer a sweeter, sour, drier, or fruity cocktail?\n",
    "2. Would you like your cocktail shaken, muddled, or stirred?\n",
    "3. Which type of distilled alcohol do you favor (e.g., whisky, gin, vodka, etc.) or any fermented beverages?\n",
    "4. Any additional ingredients that you would like or dislike?\n",
    "\n",
    "CORRECT USAGE: Use the AskHuman tool with the 'question' parameter.\n",
    "INCORRECT USAGE: Directly asking, \"What type of cocktail do you prefer?\"\n",
    "\n",
    "After gathering all the necessary details, you MUST use the AskHuman tool to inform the user that you have collected all the essential information to prepare the cocktail and ask if you may proceed with generating the cocktail recipe. \\\n",
    "\n",
    "If the user confirms, YOU MUST use the AskHuman tool to provide the unique cocktail recipe; if the user requests changes, ask follow-up questions using the AskHuman tool to clarify their modifications.\n",
    "\n",
    "Your cocktail recipe must include:\n",
    "- An original cocktail name.\n",
    "- A detailed list of ingredients with precise measurements.\n",
    "- Step-by-step preparation instructions.\n",
    "- A serving suggestion (including glass type, garnish, etc.).\n",
    "- Message asking if the user approveds the cocktail.\n",
    "\n",
    "Use the AskHuman tool to generate the unique cocktail and ask for the user's approval on the cocktail. If the user explicitly approves the cocktail then only return the message 'END' without any tool calls. \n",
    "\n",
    "Ensure that the conversation remains active and the cocktail recipe process only finalizes once the user has explicitly approved the cocktail.\n",
    "Always use the AskHuman tool when interacting with and presenting your cocktail to the user.\n",
    "\n",
    "Once the user approves your generated cocktail, present a kind message including the final cocktail recipe, do not use any tool calls.\n",
    "\"\"\"\n",
    "\n",
    "# SYSTEM_PROMPT = \"\"\"You are a professional cocktail designer.\n",
    "\n",
    "# IMPORTANT: You MUST handle all user communications exclusively via the AskHuman tool. NEVER send plain text responses.\n",
    "\n",
    "# Procedure:\n",
    "# 1. Ask the following questions ONE AT A TIME using the AskHuman tool. ALWAYS WAIT FOR THE USER'S RESPONSE BEFORE ASKING THE NEXT QUESTION:\n",
    "#    a. Do you prefer a sweeter, sour, drier, or fruity cocktail?\n",
    "#    b. Would you like your cocktail shaken, muddled, or stirred?\n",
    "#    c. Which distilled spirit (e.g., whisky, gin, vodka) or fermented beverage do you favor?\n",
    "#    d. Are there any additional ingredients you want to include or avoid?\n",
    "\n",
    "# 2. After collecting all necessary details, inform the user (via AskHuman) that you have all the required information and ask if you should proceed with creating their custom cocktail recipe. If user requests any modifications, follow them accordingly and communicate with the user via the AskHuman tool.\n",
    "\n",
    "# 3. Final Cocktail Recipe:\n",
    "#    - Construct a complete recipe that includes:\n",
    "#        ‚Ä¢ An original cocktail name\n",
    "#        ‚Ä¢ A detailed list of ingredients with exact measurements\n",
    "#        ‚Ä¢ Step-by-step preparation instructions\n",
    "#        ‚Ä¢ A serving suggestion (including glass type and garnish)\n",
    "#    - Present the entire recipe together with an approval query (e.g., ‚ÄúDo you approve of this cocktail?‚Äù) in a single AskHuman call.\n",
    "#    - NEVER output the recipe as plain text.\n",
    "\n",
    "# 4. If the user approves the cocktail, reply with a kind closing message that includes the cocktail details and conclude the flow without invoking any tools. If the user does not approve, continue refining the cocktail using the AskHuman tool.\n",
    "\n",
    "# Follow these guidelines exactly in every interaction.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Help me build a cocktail!\n",
      "\n",
      "\n",
      "THIS IS THE FULL RESPONSE FROM MODEL.INVOKE:\n",
      " content='' additional_kwargs={'tool_calls': [{'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'function': {'arguments': '{\"question\":\"Do you prefer a sweeter, sour, drier, or fruity cocktail?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 506, 'total_tokens': 536, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-eaef3d20-e1e6-4f3b-bfd3-41dc677b0ed0-0' tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Do you prefer a sweeter, sour, drier, or fruity cocktail?'}, 'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'type': 'tool_call'}] usage_metadata={'input_tokens': 506, 'output_tokens': 30, 'total_tokens': 536, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<><><><><>\n",
      "THIS IS THE LAST MESSAGE: content='' additional_kwargs={'tool_calls': [{'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'function': {'arguments': '{\"question\":\"Do you prefer a sweeter, sour, drier, or fruity cocktail?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 506, 'total_tokens': 536, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-eaef3d20-e1e6-4f3b-bfd3-41dc677b0ed0-0' tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Do you prefer a sweeter, sour, drier, or fruity cocktail?'}, 'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'type': 'tool_call'}] usage_metadata={'input_tokens': 506, 'output_tokens': 30, 'total_tokens': 536, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}} + \n",
      "<><><><><>\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  AskHuman (call_kMIdxB6sCqRyDQbTGsGDPxa4)\n",
      " Call ID: call_kMIdxB6sCqRyDQbTGsGDPxa4\n",
      "  Args:\n",
      "    question: Do you prefer a sweeter, sour, drier, or fruity cocktail?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in app.stream(\n",
    "  {\n",
    "        \"messages\": [ (\n",
    "            \"system\",\n",
    "             SYSTEM_PROMPT\n",
    "            \n",
    "            ) ,\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Help me build a cocktail!\",  # \"Use the search tool to ask the user where they are, then look up the weather there\"\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of state:  <class 'langgraph.types.StateSnapshot'>\n",
      "{'messages': [SystemMessage(content='You are a professional cocktail designer.\\n\\nCRITICAL INSTRUCTION: ALWAYS use the AskHuman tool to ask questions. NEVER ask questions directly in your response text.\\nCRITICAL: When presenting the final cocktail recipe, you MUST use the AskHuman tool with the ENTIRE recipe text AND the approval question together in the \\'question\\' parameter. DO NOT generate a regular text message containing the recipe.\\n\\nAsk these questions ONE AT A TIME using the AskHuman tool:\\n1. Do you prefer a sweeter, sour, drier, or fruity cocktail?\\n2. Would you like your cocktail shaken, muddled, or stirred?\\n3. Which type of distilled alcohol do you favor (e.g., whisky, gin, vodka, etc.) or any fermented beverages?\\n4. Any additional ingredients that you would like or dislike?\\n\\nCORRECT USAGE: Use the AskHuman tool with the \\'question\\' parameter.\\nINCORRECT USAGE: Directly asking, \"What type of cocktail do you prefer?\"\\n\\nAfter gathering all the necessary details, you MUST use the AskHuman tool to inform the user that you have collected all the essential information to prepare the cocktail and ask if you may proceed with generating the cocktail recipe. \\nIf the user confirms, YOU MUST use the AskHuman tool to provide the unique cocktail recipe; if the user requests changes, ask follow-up questions using the AskHuman tool to clarify their modifications.\\n\\nYour cocktail recipe must include:\\n- An original cocktail name.\\n- A detailed list of ingredients with precise measurements.\\n- Step-by-step preparation instructions.\\n- A serving suggestion (including glass type, garnish, etc.).\\n- Message asking if the user approveds the cocktail.\\n\\nUse the AskHuman tool to generate the unique cocktail and ask for the user\\'s approval on the cocktail. If the user explicitly approves the cocktail then only return the message \\'END\\' without any tool calls. \\n\\nEnsure that the conversation remains active and the cocktail recipe process only finalizes once the user has explicitly approved the cocktail.\\nAlways use the AskHuman tool when interacting with and presenting your cocktail to the user.\\n\\nOnce the user approves your generated cocktail, present a kind message including the final cocktail recipe, do not use any tool calls.\\n', additional_kwargs={}, response_metadata={}, id='65f9df93-d1af-4cb4-8e56-702cac1181ef'), HumanMessage(content='Help me build a cocktail!', additional_kwargs={}, response_metadata={}, id='4f762541-50bf-421b-ab49-3836dc8c87ce'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'function': {'arguments': '{\"question\":\"Do you prefer a sweeter, sour, drier, or fruity cocktail?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 506, 'total_tokens': 536, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-eaef3d20-e1e6-4f3b-bfd3-41dc677b0ed0-0', tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Do you prefer a sweeter, sour, drier, or fruity cocktail?'}, 'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 506, 'output_tokens': 30, 'total_tokens': 536, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "thing type:  <class 'dict'>\n",
      "---\n",
      "('ask_human',)\n",
      "thing type:  <class 'tuple'>\n",
      "---\n",
      "{'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f01ac0a-9ef7-6222-8001-792eb015e5dc'}}\n",
      "thing type:  <class 'dict'>\n",
      "---\n",
      "{'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'function': {'arguments': '{\"question\":\"Do you prefer a sweeter, sour, drier, or fruity cocktail?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 506, 'total_tokens': 536, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-eaef3d20-e1e6-4f3b-bfd3-41dc677b0ed0-0', tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Do you prefer a sweeter, sour, drier, or fruity cocktail?'}, 'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 506, 'output_tokens': 30, 'total_tokens': 536, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}, 'thread_id': '2', 'step': 1, 'parents': {}}\n",
      "thing type:  <class 'dict'>\n",
      "---\n",
      "2025-04-16T12:45:23.557008+00:00\n",
      "thing type:  <class 'str'>\n",
      "---\n",
      "{'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f01ac0a-8e38-694a-8000-cf97ca64ce9c'}}\n",
      "thing type:  <class 'dict'>\n",
      "---\n",
      "(PregelTask(id='b4dbc7cf-4063-9b22-104f-19b27d9ddbe8', name='ask_human', path=('__pregel_pull', 'ask_human'), error=None, interrupts=(Interrupt(value='Do you prefer a sweeter, sour, drier, or fruity cocktail?', resumable=True, ns=['ask_human:b4dbc7cf-4063-9b22-104f-19b27d9ddbe8'], when='during'),), state=None, result=None),)\n",
      "thing type:  <class 'tuple'>\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# State of the graph:\n",
    "a = app.get_state(config)\n",
    "print(\"type of state: \", type(a))\n",
    "for thing in a:\n",
    "      print(thing)\n",
    "      print(\"thing type: \", type(thing))\n",
    "      print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ask_human',)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next step\n",
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'langchain_core.messages.ai.AIMessage'>:\n",
      "fcontent='' additional_kwargs={'tool_calls': [{'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'function': {'arguments': '{\"question\":\"Do you prefer a sweeter, sour, drier, or fruity cocktail?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 506, 'total_tokens': 536, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-eaef3d20-e1e6-4f3b-bfd3-41dc677b0ed0-0' tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Do you prefer a sweeter, sour, drier, or fruity cocktail?'}, 'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'type': 'tool_call'}] usage_metadata={'input_tokens': 506, 'output_tokens': 30, 'total_tokens': 536, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "+++++++\n",
      "type: <class 'langchain_core.messages.tool.ToolMessage'>:\n",
      "fcontent='I want a sour cocktail' id='bbabc6b5-8a64-4a20-95ee-691f59f662a4' tool_call_id='call_kMIdxB6sCqRyDQbTGsGDPxa4'\n",
      "+++++++\n",
      "\n",
      "\n",
      "THIS IS THE FULL RESPONSE FROM MODEL.INVOKE:\n",
      " content='' additional_kwargs={'tool_calls': [{'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'function': {'arguments': '{\"question\":\"Would you like your cocktail shaken, muddled, or stirred?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 548, 'total_tokens': 576, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-f681ceaa-6511-4821-b264-e516589b1b2f-0' tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Would you like your cocktail shaken, muddled, or stirred?'}, 'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'type': 'tool_call'}] usage_metadata={'input_tokens': 548, 'output_tokens': 28, 'total_tokens': 576, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<><><><><>\n",
      "THIS IS THE LAST MESSAGE: content='' additional_kwargs={'tool_calls': [{'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'function': {'arguments': '{\"question\":\"Would you like your cocktail shaken, muddled, or stirred?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 548, 'total_tokens': 576, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-f681ceaa-6511-4821-b264-e516589b1b2f-0' tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Would you like your cocktail shaken, muddled, or stirred?'}, 'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'type': 'tool_call'}] usage_metadata={'input_tokens': 548, 'output_tokens': 28, 'total_tokens': 576, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}} + \n",
      "<><><><><>\n",
      "\n",
      "\n",
      "type: <class 'langchain_core.messages.ai.AIMessage'>:\n",
      "fcontent='' additional_kwargs={'tool_calls': [{'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'function': {'arguments': '{\"question\":\"Would you like your cocktail shaken, muddled, or stirred?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 548, 'total_tokens': 576, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-f681ceaa-6511-4821-b264-e516589b1b2f-0' tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Would you like your cocktail shaken, muddled, or stirred?'}, 'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'type': 'tool_call'}] usage_metadata={'input_tokens': 548, 'output_tokens': 28, 'total_tokens': 576, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "+++++++\n"
     ]
    }
   ],
   "source": [
    "# Tool message\n",
    "for event in app.stream(Command(resume=\"I want a sour cocktail\"), config, stream_mode=\"values\"):\n",
    "    #event[\"messages\"][-1].pretty_print()\n",
    "    print(f\"type: {type(event[\"messages\"][-1])}:\\nf{event[\"messages\"][-1]}\")\n",
    "    print('+++++++')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ask_human',)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of state:  <class 'langgraph.types.StateSnapshot'>\n",
      "{'messages': [SystemMessage(content='You are a professional cocktail designer.\\n\\nCRITICAL INSTRUCTION: ALWAYS use the AskHuman tool to ask questions. NEVER ask questions directly in your response text.\\nCRITICAL: When presenting the final cocktail recipe, you MUST use the AskHuman tool with the ENTIRE recipe text AND the approval question together in the \\'question\\' parameter. DO NOT generate a regular text message containing the recipe.\\n\\nAsk these questions ONE AT A TIME using the AskHuman tool:\\n1. Do you prefer a sweeter, sour, drier, or fruity cocktail?\\n2. Would you like your cocktail shaken, muddled, or stirred?\\n3. Which type of distilled alcohol do you favor (e.g., whisky, gin, vodka, etc.) or any fermented beverages?\\n4. Any additional ingredients that you would like or dislike?\\n\\nCORRECT USAGE: Use the AskHuman tool with the \\'question\\' parameter.\\nINCORRECT USAGE: Directly asking, \"What type of cocktail do you prefer?\"\\n\\nAfter gathering all the necessary details, you MUST use the AskHuman tool to inform the user that you have collected all the essential information to prepare the cocktail and ask if you may proceed with generating the cocktail recipe. \\nIf the user confirms, YOU MUST use the AskHuman tool to provide the unique cocktail recipe; if the user requests changes, ask follow-up questions using the AskHuman tool to clarify their modifications.\\n\\nYour cocktail recipe must include:\\n- An original cocktail name.\\n- A detailed list of ingredients with precise measurements.\\n- Step-by-step preparation instructions.\\n- A serving suggestion (including glass type, garnish, etc.).\\n- Message asking if the user approveds the cocktail.\\n\\nUse the AskHuman tool to generate the unique cocktail and ask for the user\\'s approval on the cocktail. If the user explicitly approves the cocktail then only return the message \\'END\\' without any tool calls. \\n\\nEnsure that the conversation remains active and the cocktail recipe process only finalizes once the user has explicitly approved the cocktail.\\nAlways use the AskHuman tool when interacting with and presenting your cocktail to the user.\\n\\nOnce the user approves your generated cocktail, present a kind message including the final cocktail recipe, do not use any tool calls.\\n', additional_kwargs={}, response_metadata={}, id='65f9df93-d1af-4cb4-8e56-702cac1181ef'), HumanMessage(content='Help me build a cocktail!', additional_kwargs={}, response_metadata={}, id='4f762541-50bf-421b-ab49-3836dc8c87ce'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'function': {'arguments': '{\"question\":\"Do you prefer a sweeter, sour, drier, or fruity cocktail?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 506, 'total_tokens': 536, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-eaef3d20-e1e6-4f3b-bfd3-41dc677b0ed0-0', tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Do you prefer a sweeter, sour, drier, or fruity cocktail?'}, 'id': 'call_kMIdxB6sCqRyDQbTGsGDPxa4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 506, 'output_tokens': 30, 'total_tokens': 536, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='I want a sour cocktail', id='bbabc6b5-8a64-4a20-95ee-691f59f662a4', tool_call_id='call_kMIdxB6sCqRyDQbTGsGDPxa4'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'function': {'arguments': '{\"question\":\"Would you like your cocktail shaken, muddled, or stirred?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 548, 'total_tokens': 576, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f681ceaa-6511-4821-b264-e516589b1b2f-0', tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Would you like your cocktail shaken, muddled, or stirred?'}, 'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'type': 'tool_call'}], usage_metadata={'input_tokens': 548, 'output_tokens': 28, 'total_tokens': 576, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "thing type:  <class 'dict'>\n",
      "---\n",
      "('ask_human',)\n",
      "thing type:  <class 'tuple'>\n",
      "---\n",
      "{'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f01ac0d-693e-64ca-8003-506dcd90f1ac'}}\n",
      "thing type:  <class 'dict'>\n",
      "---\n",
      "{'source': 'loop', 'writes': {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'function': {'arguments': '{\"question\":\"Would you like your cocktail shaken, muddled, or stirred?\"}', 'name': 'AskHuman'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 548, 'total_tokens': 576, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f681ceaa-6511-4821-b264-e516589b1b2f-0', tool_calls=[{'name': 'AskHuman', 'args': {'question': 'Would you like your cocktail shaken, muddled, or stirred?'}, 'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'type': 'tool_call'}], usage_metadata={'input_tokens': 548, 'output_tokens': 28, 'total_tokens': 576, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}, 'thread_id': '2', 'step': 3, 'parents': {}}\n",
      "thing type:  <class 'dict'>\n",
      "---\n",
      "2025-04-16T12:46:38.454482+00:00\n",
      "thing type:  <class 'str'>\n",
      "---\n",
      "{'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f01ac0d-5f9f-6cac-8002-ccb6f14120dd'}}\n",
      "thing type:  <class 'dict'>\n",
      "---\n",
      "(PregelTask(id='44418072-770f-4090-ac2d-b58c111668d0', name='ask_human', path=('__pregel_pull', 'ask_human'), error=None, interrupts=(Interrupt(value='Would you like your cocktail shaken, muddled, or stirred?', resumable=True, ns=['ask_human:44418072-770f-4090-ac2d-b58c111668d0'], when='during'),), state=None, result=None),)\n",
      "thing type:  <class 'tuple'>\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "a = app.get_state(config)\n",
    "print(\"type of state: \", type(a))\n",
    "for thing in a:\n",
    "      print(thing)\n",
    "      print(\"thing type: \", type(thing))\n",
    "      print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'langchain_core.messages.ai.AIMessage'>:\n",
      "f[{'name': 'AskHuman', 'args': {'question': 'Would you like your cocktail shaken, muddled, or stirred?'}, 'id': 'call_tUMRgMkpCWHdzajc9rKDEMCt', 'type': 'tool_call'}]\n",
      "+++++++\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ToolMessage' object has no attribute 'tool_calls'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# response from Command(resume=) get formatted as a tool response\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m app.stream(Command(resume=\u001b[33m\"\u001b[39m\u001b[33mshaken\u001b[39m\u001b[33m\"\u001b[39m), config, stream_mode=\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m#event[\"messages\"][-1].pretty_print()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(event[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mf\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mevent\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m+++++++\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/learning/LangGraph/langgraph_personal/NodeInterrupt/myvenv/lib/python3.12/site-packages/pydantic/main.py:891\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    888\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    890\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ToolMessage' object has no attribute 'tool_calls'"
     ]
    }
   ],
   "source": [
    "# response from Command(resume=) get formatted as a tool response\n",
    "for event in app.stream(Command(resume=\"shaken\"), config, stream_mode=\"values\"):\n",
    "    #event[\"messages\"][-1].pretty_print()\n",
    "    print(f\"type: {type(event[\"messages\"][-1])}:\\nf{event[\"messages\"][-1].tool_calls}\")\n",
    "    print('+++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in app.stream(Command(resume=\"vodka\"), config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in app.stream(Command(resume=\"I want sage in my cocktail\"), config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in app.stream(Command(resume=\"Yes\"), config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in app.stream(Command(resume=\"yes\"), config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app.get_state(config).next\n",
    "app.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if app.get_state(config).next:\n",
    "      print(\"keep going\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in app.stream(Command(resume=\"I don't have any thing in mind, just another recipe\"), config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in app.stream(Command(resume=\"I approve!\"), config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = app.get_state(config)\n",
    "for thing in a:\n",
    "      print(thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINICAO DE ROTA -- FAZER ISSO NO JAVA / TYPE SCRIPT\n",
    "\n",
    "Assistente medico, 2 tools \n",
    "      1. emergencia\n",
    "            - estado do paciente sugerir pronto atendimento. ex: dor no peito, calafrio, confusao mental. Pode ser infarto. GOAL --> sugerir a internacao ou procedimento etc.. \n",
    "            - o q fazer naquele momento\n",
    "      2. diagnostico diferencial\n",
    "            - baseado numa descricao (text) do estado do paciente, investigar a causa.\n",
    "            - eh mais pra buscar causas\n",
    "\n",
    "entry prompt:\n",
    "- assiste medico, vc tem esses tools de emergencia e diagnostico. \n",
    "- se o input do usuario nao eh claro, perguntar pro usuario se √© emergencia ou diagnostico differencial.\n",
    "\n",
    "\n",
    "Caso de ambiguidade sobre quais decisoes tomar: \n",
    "      - algo mais emergencial \n",
    "      - busca investigacao \n",
    "\n",
    "\n",
    "- Human in the loop para:\n",
    "      - clarificacao ou confirmacao (confirmacao so pra fins de aprender)\n",
    "      - confirmacao seria mais pra processos de arvore de decisao\n",
    "\n",
    "TESTER API do MEMED.\n",
    "- supondo que .... gerar a receita.\n",
    "- comeca a testar com o postman\n",
    "\n",
    "\n",
    "BULARIO\n",
    "- se puxar 20 remedios, como ele vai mostrar a informacao? uma tabela? \n",
    "- batch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
